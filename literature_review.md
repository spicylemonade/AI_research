# Literature Review: Transformer-Based Symbolic Regression and Related Work for PhysDiffuser

**Date:** 2026-02-14
**Scope:** This document surveys the literature on transformer-based symbolic regression, physics-informed machine learning, masked/discrete diffusion models, reasoning architectures, and benchmarks relevant to the PhysDiffuser project. PhysDiffuser is a masked discrete diffusion transformer that iteratively refines equation token sequences to derive physics equations from observational data.

---

## (a) Transformer-Based Symbolic Regression

### 1. NeSymReS: Neural Symbolic Regression that Scales
- **Authors:** Luca Biggio, Tommaso Bendinelli, Alexander Neitz, Aurelien Lucchi, Giambattista Parascandolo
- **Year/Venue:** ICML 2021
- **Core Method:** NeSymReS introduces a set-transformer encoder paired with an autoregressive decoder for symbolic regression. The encoder uses a novel IEEE-754 multi-hot encoding scheme: each floating-point scalar in the observation data is decomposed into its 16-bit half-precision binary representation, yielding a 16-dimensional binary vector per scalar value. This representation is processed through Induced Set Attention Blocks (ISABs) that enforce permutation invariance over the observation points, producing a fixed-dimensional latent vector. The decoder is a standard autoregressive transformer that generates equation tokens in prefix notation. Crucially, NeSymReS predicts equation "skeletons" with placeholder constants (denoted C) rather than attempting to predict numerical constant values directly. After skeleton generation, a BFGS optimization step fits the constant values to the observed data.
- **Key Results:** Performance scales consistently with pre-training dataset size: exact match accuracy on the AI Feynman benchmark rises from approximately 0.2 with 10K training equations to greater than 0.7 with 10M training equations. The encoder comprises approximately 11M parameters and the decoder approximately 13M parameters, totaling roughly 24M parameters. The model achieves competitive results with GP-based methods while being orders of magnitude faster at inference time (seconds vs. hours). On the Feynman benchmark, NeSymReS recovers 90+ equations at the 10M data scale.
- **Relevance to PhysDiffuser:** Our encoder architecture directly builds on NeSymReS's set-transformer combined with IEEE-754 multi-hot encoding. The permutation-invariant encoding of observation sets via ISABs is adopted as our observation encoder. We also follow their approach of generating prefix-notation token sequences. However, PhysDiffuser replaces the autoregressive decoder with a masked diffusion process, enabling iterative bidirectional refinement rather than left-to-right sequential generation.

---

### 2. End-to-End Symbolic Regression with Transformers (E2E-Transformer)
- **Authors:** Pierre-Alexandre Kamienny, Stephane d'Ascoli, Guillaume Lample, Francois Charton
- **Year/Venue:** NeurIPS 2022
- **Core Method:** This work extends the transformer-based symbolic regression paradigm to an end-to-end approach that predicts full mathematical expressions including numerical constant values, not just equation skeletons with placeholders. The model architecture consists of an encoder that processes observation data and a decoder that generates the symbolic expression token by token, with constant values emitted as special tokens followed by their numerical representations. After the transformer generates an initial expression with predicted constants, a BFGS nonlinear optimization step refines the constant values to better fit the observed data.
- **Key Results:** The key finding is that initializing BFGS with the model's predicted constant values significantly outperforms random initialization of the constants. On the AI Feynman benchmark, the E2E approach with BFGS refinement achieves higher recovery rates than skeleton-based methods, particularly on equations with multiple constants where the optimization landscape has many local minima. The model-predicted constants provide a warm start that guides BFGS toward the correct basin of attraction. The approach achieves state-of-the-art results at the time of publication on the standard Feynman benchmark suite, recovering approximately 78% of equations with R-squared above 0.999.
- **Relevance to PhysDiffuser:** We adopt the E2E constant prediction plus BFGS refinement pipeline from this work. In PhysDiffuser, the masked diffusion process generates full expressions including initial constant estimates, which are then refined using BFGS optimization. The finding that learned constant initialization dramatically improves optimization outcomes directly motivates our end-to-end constant prediction design.

---

### 3. TPSR: Transformer-based Planning for Symbolic Regression
- **Authors:** Parshin Shojaee, Kazem Meidani, Amir Barati Farimani, Chandan K. Reddy
- **Year/Venue:** NeurIPS 2023
- **Core Method:** TPSR integrates Monte Carlo Tree Search (MCTS) into the transformer decoding process for symbolic regression. Rather than relying solely on beam search or greedy decoding, TPSR uses the transformer's token probability distribution as a policy within MCTS. At each step of equation generation, MCTS explores multiple possible continuations, evaluating partial expressions using non-differentiable feedback signals including fitting accuracy on the observation data and expression complexity penalties. The search balances exploration (trying novel operator-variable combinations) and exploitation (extending high-scoring partial equations) through the UCB1 selection criterion.
- **Key Results:** TPSR ranked 1st in the 2023 SRBench Competition, demonstrating the substantial value of search-based refinement beyond standard beam search. On the AI Feynman benchmark, TPSR recovers approximately 10-15% more equations than the base transformer with beam search alone. The MCTS integration adds inference-time computational cost (approximately 5-10x slowdown compared to beam search), but the accuracy gains are substantial, particularly on complex multi-variable equations where the search space is vast. The method also demonstrates improved Pareto-optimal trade-offs between expression accuracy and complexity.
- **Relevance to PhysDiffuser:** TPSR demonstrates the significant value of iterative search-based refinement during inference. Our masked diffusion iterative refinement process serves a conceptually similar role: rather than generating an equation in a single left-to-right pass, PhysDiffuser iteratively refines all token positions simultaneously over multiple denoising steps. The masked diffusion approach can be viewed as an implicit search process where each denoising step explores the space of possible token assignments at masked positions. This parallel iterative refinement potentially offers advantages over MCTS's sequential tree exploration.

---

### 4. ODEFormer: Symbolic Regression of Dynamical Systems with Transformers
- **Authors:** Stephane d'Ascoli, Sebastien Lachapelle, Philippe-Alexandre Brouillard, Tristan Deleu, Dhanya Sridhar, Guillaume Lajoie, Pierre-Alexandre Kamienny, Alexandre Drouin, Simon Lacoste-Julien
- **Year/Venue:** ICLR 2024
- **Core Method:** ODEFormer is the first transformer-based method designed specifically for recovering symbolic representations of multidimensional ordinary differential equation (ODE) systems from observational data. The model takes as input time-series observations of a dynamical system (multiple state variables observed at potentially irregular time points) and outputs the symbolic ODE governing the system's evolution. The architecture is a large-scale encoder-decoder transformer with 86M parameters, trained on 50M synthetically generated ODE systems. The encoder processes the irregularly sampled time-series observations while the decoder generates the ODE system equations in a structured format.
- **Key Results:** ODEFormer achieves robust performance on noisy and irregularly sampled observations, a crucial capability for real-world scientific applications. On the ODE-Strogatz benchmark, it recovers the correct ODE system for approximately 65% of test cases. The model demonstrates particular strength on systems of coupled ODEs (2-4 dimensions), where traditional SR methods often struggle due to the combinatorial complexity. The 86M parameter model, trained on 50M synthetic equations, represents the largest-scale transformer for symbolic regression to date. Notably, the model shows graceful degradation under noise: at 1% Gaussian noise, performance drops by less than 5%, and even at 10% noise, recovery rates remain above 40%.
- **Relevance to PhysDiffuser:** ODEFormer serves as a benchmark comparison target and demonstrates that transformer-based symbolic regression can scale effectively. Its success with irregularly sampled, noisy observations validates the general approach of using transformers for scientific equation discovery. PhysDiffuser targets a related but distinct problem (algebraic physics equations rather than ODE systems), and we compare against ODEFormer's published numbers where applicable. The scale of ODEFormer (86M params, 50M training examples) also informs our model sizing decisions.

---

### 5. SymFormer: End-to-end Symbolic Regression
- **Authors:** Martin Vastl, Jonas Kulhanek, Jiri Kubalik, Erik Derner, Robert Babuska
- **Year/Venue:** IEEE Access 2024
- **Core Method:** SymFormer proposes another end-to-end approach to symbolic regression where the transformer simultaneously predicts formula symbols (operators, variables) and numerical constant values within a unified sequence. Constants are encoded as floating-point tokens that the model predicts alongside the symbolic tokens. After the initial prediction, the predicted constant values serve as initialization points for gradient descent refinement, optimizing the constants to minimize the mean squared error on the observation data.
- **Key Results:** SymFormer achieves competitive results on standard benchmarks, demonstrating that simultaneous symbol-constant prediction is viable without the explicit skeleton-then-fit pipeline. On the AI Feynman benchmark subset used for evaluation, SymFormer achieves approximately 60-70% recovery rate, with the gradient descent refinement step improving raw model predictions by 10-20% in terms of equations recovered. The model is relatively lightweight compared to ODEFormer, enabling faster inference times suitable for interactive use.
- **Relevance to PhysDiffuser:** SymFormer represents another approach to E2E constant prediction that we compare against. Its use of gradient descent (rather than BFGS) for constant refinement provides an alternative optimization strategy. In PhysDiffuser, we adopt the BFGS approach from Kamienny et al. (2022) rather than gradient descent, but SymFormer's results confirm that jointly predicting structure and constants is a sound design choice.

---

## (b) Physics-Informed Machine Learning

### 6. AI Feynman: A Physics-Inspired Method for Symbolic Regression
- **Authors:** Silviu-Marian Udrescu, Max Tegmark
- **Year/Venue:** Science Advances 2020
- **Core Method:** AI Feynman is a recursive algorithm that combines neural network function fitting with physics-inspired decomposition techniques for symbolic regression. The algorithm operates in multiple stages: first, a neural network is trained to fit the data, providing a smooth approximation. Then, a series of physics-inspired tests are applied to discover structural properties of the underlying function. These tests include: (1) dimensional analysis to constrain the space of possible expressions by ensuring physical units are consistent; (2) translational and rotational symmetry detection to identify invariances; (3) separability testing to determine if the function factors into independent sub-functions of disjoint variable sets; (4) compositionality testing to determine if the function can be decomposed as f(g(x), h(y)); and (5) generalized symmetry detection using neural network gradients. When a structural property is identified, the problem is recursively decomposed into simpler sub-problems.
- **Key Results:** AI Feynman discovers all 100 equations from the Feynman Lectures on Physics dataset, compared to 71 for the best prior method (Eureqa). The physics-inspired decomposition techniques are essential: without them, the baseline neural network approach solves fewer than 30 equations. The dimensional analysis module alone accounts for recovering approximately 15 additional equations. The separability and compositionality tests enable the solution of equations with 5+ variables that are intractable for flat GP-based search. The approach establishes the Feynman Symbolic Regression Database as the standard benchmark for the field.
- **Relevance to PhysDiffuser:** AI Feynman defines our primary benchmark (the Feynman Symbolic Regression Database). More importantly, the physics-inspired priors demonstrated in this work directly motivate PhysDiffuser's physics-informed structural priors module. We incorporate dimensional analysis as an auxiliary loss function, and the separability and symmetry concepts inform our data augmentation strategy. The key insight from AI Feynman -- that physics-specific inductive biases dramatically improve equation discovery -- is central to our approach.

---

### 7. AI Feynman 2.0: Pareto-Optimal Symbolic Regression Exploiting Graph Modularity
- **Authors:** Silviu-Marian Udrescu, Andrew Tan, Jiahai Feng, Orisvaldo Neto, Tailin Wu, Max Tegmark
- **Year/Venue:** NeurIPS 2020
- **Core Method:** AI Feynman 2.0 extends the original algorithm with two major innovations. First, it introduces graph modularity exploitation: the algorithm constructs a graph where variables are nodes and edges represent detected functional dependencies, then uses community detection to identify modular sub-structures that can be solved independently. Second, it introduces Pareto-optimal symbolic regression, which searches for expressions that achieve the best trade-off between fitting accuracy and expression complexity (measured by description length). The Pareto front allows the algorithm to avoid overfitting by preferring simpler expressions when they achieve comparable accuracy.
- **Key Results:** AI Feynman 2.0 improves the success rate on the difficult subset of Feynman equations from 15% to 90%. The graph modularity technique is particularly effective for equations with 6+ variables, where it correctly identifies independent sub-groups of variables, reducing the effective search space exponentially. The Pareto-optimal complexity analysis adds an additional 12 equations that the original AI Feynman missed due to overfitting with overly complex expressions. The extended benchmark includes additional equations beyond the original 100, providing a more comprehensive evaluation suite.
- **Relevance to PhysDiffuser:** AI Feynman 2.0 extends our benchmark equations and provides the complexity analysis framework we use for difficulty tiering. The Pareto-optimal accuracy-complexity trade-off directly informs PhysDiffuser's expression complexity penalty during masked diffusion sampling. The graph modularity concept is related to our compositionality prior, which attempts to decompose complex equations into simpler sub-expression derivation chains.

---

### 8. Physics-Informed Neural Networks (PINNs)
- **Authors:** Maziar Raissi, Paris Perdikaris, George Em Karniadakis
- **Year/Venue:** Journal of Computational Physics, 2019
- **Core Method:** PINNs embed known physical laws (expressed as partial differential equations) directly into the loss function used to train neural networks. Given a PDE of the form F(u, du/dx, du/dt, d2u/dx2, ...) = 0, a standard neural network u_theta(x,t) is trained to satisfy both the PDE (evaluated at collocation points in the domain interior) and the boundary/initial conditions (evaluated at boundary points). The total loss is a weighted sum: L = L_data + lambda_PDE * L_PDE + lambda_BC * L_BC, where L_PDE measures the residual of the governing PDE at collocation points. This approach enables the neural network to respect known physics even when trained on sparse or noisy data.
- **Key Results:** PINNs demonstrate that encoding physical constraints dramatically reduces the data requirements for accurate function approximation. On benchmark PDEs (Burgers' equation, Schrodinger equation, Navier-Stokes), PINNs achieve accurate solutions with orders of magnitude fewer data points than purely data-driven approaches. The physics-informed loss acts as a strong regularizer, preventing overfitting and enabling extrapolation beyond the training domain. The paper has been cited over 10,000 times and spawned an extensive literature on physics-informed machine learning.
- **Relevance to PhysDiffuser:** Our dimensional analysis loss is directly inspired by the PINN framework of encoding physical constraints as auxiliary loss terms. While PINNs encode known PDEs, PhysDiffuser encodes dimensional consistency as a soft constraint during training. The key insight transferred from PINNs is that physics-based loss terms can serve as powerful regularizers that improve generalization and reduce data requirements for scientific machine learning models.

---

## (c) Masked Diffusion / Discrete Diffusion Models

### 9. LLaDA: Large Language Diffusion with mAsking
- **Authors:** Nie et al.
- **Year/Venue:** arXiv 2025
- **Core Method:** LLaDA is a large-scale masked diffusion language model with 8 billion parameters that applies the masked diffusion framework to text generation. The forward diffusion process progressively masks tokens in the input sequence: at diffusion time t, each token is independently replaced with a [MASK] token with probability t. The reverse process is a transformer that predicts the original token at each masked position, conditioned on the remaining unmasked tokens. Training uses a variable masking ratio, where the diffusion time t is sampled uniformly from U[0,1] for each training example, exposing the model to all levels of corruption. At inference time, generation proceeds from a fully masked sequence (all [MASK] tokens) through iterative unmasking: at each step, the model predicts tokens at all masked positions, and a subset of the most confident predictions are unmasked (committed), with the remaining positions re-masked for the next iteration.
- **Key Results:** Despite using a fundamentally different generation paradigm (parallel iterative refinement vs. autoregressive left-to-right), LLaDA achieves performance competitive with LLaMA3 8B on a range of downstream NLP tasks including reading comprehension, commonsense reasoning, and mathematical problem solving. On the MMLU benchmark, LLaDA-8B scores within 2-3% of LLaMA3-8B. The model demonstrates that masked diffusion can scale to billions of parameters and compete with autoregressive models that have benefited from years of optimization. The variable masking ratio training is critical: models trained with fixed masking ratios perform significantly worse.
- **Relevance to PhysDiffuser:** LLaDA is the core architectural inspiration for PhysDiffuser. We adapt LLaDA's masked diffusion framework from natural language to symbolic expression generation. Our forward process masks equation tokens, and our reverse process iteratively unmasks them to generate physics equations. We adopt the variable masking ratio training scheme (t ~ U[0,1]) directly from LLaDA. The key adaptation is that PhysDiffuser's transformer is conditioned on the encoder latent vector from observation data, whereas LLaDA operates unconditionally on text.

---

### 10. MD4: Simplified and Generalized Masked Diffusion for Discrete Data
- **Authors:** Jiacheng Shi, Linyuan Gong, Alec Radford, Scott Gray
- **Year/Venue:** NeurIPS 2024
- **Core Method:** MD4 provides a unified theoretical framework for masked diffusion models over discrete data. The key contribution is showing that the continuous-time variational lower bound (VLB) for masked diffusion reduces to a weighted integral of cross-entropy losses over different masking ratios. Specifically, the training objective is: L = integral from 0 to 1 of w(t) * E[CE(x_true, f_theta(x_masked_t))] dt, where w(t) is a weighting function determined by the masking schedule and x_masked_t is the input with masking ratio t. MD4 also introduces state-dependent masking schedules, where the probability of masking a token depends on its identity or position, enabling more flexible corruption processes tailored to the data structure.
- **Key Results:** MD4 surpasses all prior discrete diffusion language models at GPT-2 scale (117M-345M parameters), achieving perplexity competitive with well-tuned autoregressive baselines. The unified framework enables fair comparison of different masking schedules and weighting functions, revealing that the commonly used uniform weighting is suboptimal. The state-dependent masking schedules provide modest improvements (1-3% perplexity reduction) on structured data where certain token positions carry more information. The theoretical analysis clarifies the relationship between masked diffusion and other discrete diffusion frameworks (D3PM, absorbing state models).
- **Relevance to PhysDiffuser:** MD4 provides the theoretical foundation for our masked diffusion training objective. We implement the continuous-time VLB as our primary training loss, using the integral-of-cross-entropies formulation. The state-dependent masking concept is relevant to PhysDiffuser because equation tokens have varying importance: operators at the root of the expression tree carry more structural information than leaf variables, suggesting that masking schedules that protect high-information tokens during early diffusion steps could improve performance.

---

### 11. MDLM: Simple and Effective Masked Diffusion Language Models
- **Authors:** Subham Sekhar Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin T. Chiu, Alexander Rush, Volodymyr Kuleshov
- **Year/Venue:** NeurIPS 2024
- **Core Method:** MDLM provides a practical, clean implementation of masked diffusion for language modeling that achieves strong performance through careful engineering choices rather than novel theoretical contributions. The model uses a cosine masking schedule (masking rate increases following a cosine curve over diffusion time), a standard transformer backbone, and a simplified training procedure that samples masking ratios and trains the model to predict masked tokens via cross-entropy loss. The key engineering contributions include: proper loss weighting across different masking ratios, efficient batching of different masking levels, and a carefully tuned inference procedure with adaptive step sizing.
- **Key Results:** MDLM achieves perplexity of 31.7 on OpenWebText at the GPT-2 small scale (117M parameters), which is competitive with autoregressive baselines and significantly better than earlier discrete diffusion models (which typically had perplexities above 40). The model scales predictably: larger variants (345M, 774M parameters) show consistent improvements following scaling laws similar to those observed for autoregressive models. The clean training procedure makes MDLM highly reproducible, with minimal hyperparameter sensitivity once the masking schedule and loss weighting are properly configured.
- **Relevance to PhysDiffuser:** MDLM serves as our primary implementation reference for the masked diffusion training procedure. We adapt their cosine masking schedule, loss weighting scheme, and inference procedure to the symbolic expression domain. The practical engineering choices documented in MDLM (learning rate schedules, batch construction, gradient clipping) directly inform our training configuration.

---

### 12. On the Reasoning Abilities of Masked Diffusion Language Models
- **Authors:** Viliam Svete, Ashish Sabharwal
- **Year/Venue:** arXiv 2025
- **Core Method:** This theoretical work analyzes the computational expressiveness of masked diffusion models (MDMs) by relating them to established models of computation. The central result establishes that MDMs with T denoising steps are equivalent in computational power to padded looped transformers with T iterations. Specifically, any function computable by a chain-of-thought (CoT)-augmented autoregressive transformer with T reasoning steps can also be computed by an MDM with T denoising steps. The proof constructs an explicit simulation: each MDM denoising step can simulate one step of CoT reasoning by "unmasking" the tokens that correspond to the next reasoning step's output. Furthermore, the authors show that MDMs can be strictly more efficient than CoT transformers on certain problem classes, particularly problems involving regular languages and pattern completion, because MDMs can refine multiple positions in parallel.
- **Key Results:** The equivalence result provides formal guarantees that MDMs are at least as powerful as CoT-augmented transformers for reasoning tasks. The efficiency advantage is demonstrated concretely: for recognizing certain regular languages, an MDM requires O(1) denoising steps regardless of input length, whereas a CoT transformer requires O(n) steps. For the ARC-style pattern completion tasks, MDMs can exploit spatial parallelism to fill in multiple grid cells simultaneously, while CoT must generate them sequentially. The paper also provides negative results, showing that without sufficient denoising steps, MDMs cannot solve problems that require deep sequential reasoning.
- **Relevance to PhysDiffuser:** This paper provides theoretical justification that masked diffusion models are capable of performing the complex symbolic reasoning required for equation derivation. The equivalence to CoT transformers is particularly relevant because equation derivation can be viewed as a multi-step reasoning process (identify functional form, determine variable relationships, compose sub-expressions). The parallel refinement advantage of MDMs is directly applicable to equation generation, where multiple tokens in the expression can be refined simultaneously based on mutual consistency constraints.

---

## (d) ARC/Reasoning Architectures

### 13. The ARChitects ARC 2025 Solution
- **Authors:** Lambda Labs (team: ARChitects)
- **Year/Venue:** ARC Prize 2025 Competition
- **Core Method:** The ARChitects solution adapts LLaDA-8B for the ARC (Abstraction and Reasoning Corpus) challenge, which requires models to infer abstract transformation rules from input-output grid pairs. The architecture incorporates three key innovations beyond base LLaDA: (1) 2D Rotary Positional Embeddings (2D RoPE) that encode the spatial structure of ARC grids, replacing the standard 1D positional encoding with separate frequency components for row and column positions; (2) Token algebra soft-masking, where learnable mask embedding vectors are added to all token positions (not just masked positions) during the iterative refinement process, allowing the model to express uncertainty and refinement signals through the soft mask channel; (3) Test-time LoRA finetuning, where low-rank adapters are added to the model at inference time and trained on the specific test example using a self-supervised objective (mask-and-predict on the input-output demonstration pairs).
- **Key Results:** The ARChitects solution achieves 21.67% accuracy on the ARC challenge, a significant achievement for what is considered one of the hardest reasoning benchmarks in AI. The most-visited-candidate selection strategy (running multiple inference trajectories and selecting the output that appears most frequently) provides approximately 3-5% absolute improvement over single-trajectory inference. The test-time LoRA finetuning contributes approximately 5-8% improvement, demonstrating that per-example adaptation is highly effective for reasoning tasks. The soft-masking refinement enables progressive improvement of the output over 50-100 denoising steps, with the model's confidence increasing monotonically across steps.
- **Relevance to PhysDiffuser:** The ARChitects solution is the direct architectural inspiration for PhysDiffuser. We adapt their three key innovations to the symbolic regression domain: (1) we replace 2D RoPE with 1D positional encodings suited to sequence-structured expressions (though we incorporate expression tree-depth positional information); (2) we adopt token algebra soft-masking for iterative equation refinement, allowing the model to express and resolve uncertainty at all token positions; (3) we implement test-time LoRA finetuning where the model adapts to each specific observation dataset before generating the final equation. The most-visited-candidate selection across multiple refinement trajectories is directly adopted for our inference pipeline.

---

### 14. LoRA: Low-Rank Adaptation of Large Language Models
- **Authors:** Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen
- **Year/Venue:** ICLR 2022
- **Core Method:** LoRA introduces a parameter-efficient fine-tuning method that freezes the pretrained model weights and injects trainable rank-decomposition matrices into each layer of the transformer architecture. For a pretrained weight matrix W of dimension d x k, LoRA adds a low-rank update: W' = W + BA, where B is d x r and A is r x k, with rank r much smaller than min(d,k) (typically r = 4, 8, or 16). During fine-tuning, only B and A are trained while W remains frozen. At inference time, the LoRA weights can be merged into the base weights (W' = W + BA) with zero additional latency, or kept separate for efficient switching between tasks.
- **Key Results:** LoRA reduces the number of trainable parameters by up to 10,000x compared to full fine-tuning while matching or exceeding the performance of full fine-tuning on many tasks. On GPT-3 175B, LoRA with rank 4 (requiring only 4.7M trainable parameters vs. 175B total) matches full fine-tuning performance on several NLU benchmarks. The low rank of the adaptation matrices implies that task-specific weight updates have a low "intrinsic dimensionality," meaning that the information needed to adapt a pretrained model to a new task can be captured in a very small number of parameters. Training LoRA adapters is approximately 3x faster than full fine-tuning due to the reduced gradient computation.
- **Relevance to PhysDiffuser:** Our test-time adaptation mechanism uses rank-8 LoRA adapters applied to the query and value projection matrices in all transformer layers. At inference time, for each new equation to be derived, we initialize fresh LoRA adapters and train them for 32 steps using a self-supervised mask-and-predict objective on the current best-guess equation and observation data. The extremely low parameter count of LoRA adapters (approximately 500K parameters for rank 8 across all layers) makes per-equation adaptation feasible even on CPU within our 20-second test-time adaptation budget.

---

## (e) Benchmarks and Baselines

### 15. PySR: High-Performance Symbolic Regression in Python and Julia
- **Authors:** Miles Cranmer
- **Year/Venue:** arXiv 2023
- **Core Method:** PySR is a high-performance symbolic regression system based on genetic programming, implemented in Julia with a Python interface. The core algorithm is a multi-population evolutionary strategy: multiple independent populations of candidate expressions evolve in parallel, periodically exchanging high-fitness individuals (island model). Within each population, PySR applies an evolve-simplify-optimize loop: (1) evolve candidate expressions through mutation and crossover operators; (2) simplify expressions algebraically using SymPy to remove redundancies; (3) optimize numerical constants in each expression using the BFGS algorithm. The Pareto front of accuracy vs. complexity is maintained throughout evolution, providing a range of solutions from simple approximations to complex exact fits.
- **Key Results:** PySR is consistently among the top-performing GP-based symbolic regression methods on standard benchmarks. On the SRBench evaluation, PySR achieves the highest recovery rate among GP methods, solving approximately 60% of Feynman benchmark equations. On simpler equations (1-3 variables, up to 5 operators), PySR approaches near-perfect recovery rates. However, performance degrades on complex equations with many variables and operators, where the evolutionary search space becomes intractably large. PySR's inference time is highly variable: simple equations are found in seconds, while complex equations may require hours of evolutionary search. PySR is the most widely used open-source symbolic regression tool, with extensive community adoption in the physical sciences.
- **Relevance to PhysDiffuser:** PySR serves as our primary baseline comparison from the genetic programming paradigm. We compare PhysDiffuser against PySR to demonstrate the advantages of the neural transformer approach, particularly on complex multi-variable equations where GP-based search becomes intractable. PySR also defines the state-of-the-art for GP-based SR, making it the most appropriate non-neural baseline.

---

### 16. Contemporary Symbolic Regression Methods and their Relative Performance (SRBench)
- **Authors:** William La Cava, Patrick Orzechowski, Bogdan Burlacu, Fabricio Olivetti de Franca, Marco Virgolin, Ying Jin, Michael Kommenda, Jason H. Moore
- **Year/Venue:** NeurIPS 2021 (Datasets and Benchmarks Track)
- **Core Method:** SRBench provides a unified, standardized benchmarking framework for symbolic regression methods. The benchmark combines two major datasets: the Feynman Symbolic Regression Database (SRD), containing equations from the Feynman Lectures on Physics, and the ODE-Strogatz dataset, containing dynamical systems equations from Strogatz's textbook. SRBench defines a standardized evaluation protocol: fixed train/test splits, standardized noise levels, unified accuracy metrics (R-squared, symbolic solution rate), and a complexity measure (number of nodes in the expression tree). The benchmark evaluates 14 symbolic regression methods under identical conditions, including GP-based methods (PySR, gplearn, Operon), neural methods (NeSymReS, DSR), and hybrid approaches (AI Feynman).
- **Key Results:** The benchmark reveals that no single method dominates across all problem types: GP-based methods excel on low-dimensional problems while neural methods show advantages on higher-dimensional problems. The symbolic solution rate (exact equation recovery) varies dramatically across methods: from less than 10% for basic GP implementations to over 70% for AI Feynman on the Feynman SRD. The standardized evaluation reveals that many published results are not directly comparable due to differences in preprocessing, noise levels, and evaluation criteria. The ODE-Strogatz component adds evaluation of dynamical system recovery, which proves significantly harder than static equation recovery for most methods.
- **Relevance to PhysDiffuser:** We follow SRBench's evaluation protocol for our experiments, ensuring that our results are directly comparable with the 14+ methods evaluated in the benchmark. We use their standardized train/test splits, noise levels, and accuracy metrics. Our Feynman benchmark evaluation (120 equations across 5 difficulty tiers) extends the SRBench Feynman SRD with additional difficulty categorization.

---

### 17. Set Transformer: A Framework for Attention-based Permutation-Invariant Input Processing
- **Authors:** Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, Yee Whye Teh
- **Year/Venue:** ICML 2019
- **Core Method:** The Set Transformer introduces attention-based architectures for processing set-structured inputs (unordered collections of elements) in a permutation-invariant manner. The key architectural contribution is the Induced Set Attention Block (ISAB), which reduces the computational complexity of self-attention over sets from O(n^2) to O(nm), where n is the set size and m is a fixed number of "inducing points." ISAB works by first projecting the set elements into m inducing point representations via multi-head attention, then projecting back from the inducing points to the set elements. For producing fixed-size set representations, the Set Transformer introduces Pooling by Multihead Attention (PMA), which uses a set of learnable "seed" vectors that attend to the set elements, producing k output vectors regardless of the input set size.
- **Key Results:** The Set Transformer achieves state-of-the-art results on set-structured tasks including point cloud classification, set anomaly detection, and amortized clustering. The ISAB mechanism enables processing of large sets (thousands of elements) that would be computationally infeasible with standard O(n^2) self-attention. With m = 32 inducing points, the Set Transformer achieves comparable accuracy to full self-attention while being 10-50x faster for large sets. PMA provides a principled alternative to simple mean/max pooling for set aggregation, capturing richer set-level statistics through multi-head attention.
- **Relevance to PhysDiffuser:** Our observation encoder uses the ISAB and PMA mechanisms from the Set Transformer. The observation data for each equation (a set of (x, y) pairs) is naturally an unordered set, making the Set Transformer's permutation-invariant processing appropriate. We use ISABs with 32 inducing points to process up to 500 observation points efficiently, followed by PMA to produce a single fixed-dimensional latent vector z that conditions the masked diffusion decoder. This architecture choice follows directly from NeSymReS, which first demonstrated the effectiveness of Set Transformers for encoding observation data in symbolic regression.

---

## Key Takeaways for PhysDiffuser Design

The following five insights, drawn from the surveyed literature, are the most important for guiding the design and development of PhysDiffuser:

### 1. Masked Diffusion is a Theoretically Sound and Practically Competitive Alternative to Autoregressive Generation

The combined evidence from LLaDA (paper 9), MD4 (paper 10), MDLM (paper 11), and the theoretical analysis of Svete & Sabharwal (paper 12) establishes that masked diffusion models are not merely a novelty but a principled alternative to autoregressive generation. LLaDA demonstrates competitive performance with LLaMA3 at 8B scale, while the theoretical equivalence to chain-of-thought transformers guarantees sufficient computational expressiveness for complex reasoning tasks like equation derivation. The parallel refinement capability of masked diffusion -- where all token positions are updated simultaneously at each denoising step -- is particularly well-suited to symbolic expressions, where mutual consistency constraints between operators and operands are bidirectional (a parent operator constrains its children, and children constrain the parent). This bidirectional dependency is poorly served by left-to-right autoregressive generation.

### 2. End-to-End Constant Prediction with Optimization-Based Refinement is the Dominant Paradigm

The progression from NeSymReS's skeleton-based approach (paper 1) through E2E-Transformer (paper 2) and SymFormer (paper 5) reveals a clear trend: predicting full expressions including constants, then refining constants with optimization (BFGS or gradient descent), consistently outperforms skeleton-only approaches. The critical insight from Kamienny et al. is that learned constant initialization dramatically improves optimization outcomes by placing the starting point near the correct basin of attraction. PhysDiffuser should adopt this pipeline: the masked diffusion process generates complete expressions with initial constant estimates, followed by BFGS refinement.

### 3. Iterative Refinement and Search are Essential for Complex Equations

TPSR's victory in the 2023 SRBench competition (paper 3) and the ARChitects' ARC solution (paper 13) both demonstrate that single-pass generation is insufficient for complex reasoning tasks. TPSR's MCTS adds 10-15% accuracy through iterative search, while the ARChitects' most-visited-candidate strategy adds 3-5% through multiple trajectory sampling. For PhysDiffuser, the masked diffusion process provides a natural framework for iterative refinement: each denoising step refines the equation, and running multiple inference trajectories with candidate selection further improves robustness. This iterative refinement capability is the primary architectural advantage of masked diffusion over standard autoregressive symbolic regression approaches.

### 4. Physics-Informed Priors Provide Dramatic Performance Improvements

AI Feynman (paper 6) and AI Feynman 2.0 (paper 7) demonstrate that incorporating physics-specific inductive biases -- dimensional analysis, symmetry detection, separability, compositionality -- can improve equation recovery rates by 30-75% on challenging equations. PINNs (paper 8) show that encoding physical constraints as loss terms is an effective and differentiable way to inject domain knowledge. For PhysDiffuser, the dimensional analysis auxiliary loss serves as a differentiable physics prior during training, while symmetry-aware data augmentation expands the effective training distribution. These priors are especially critical for the complex and multi-step equation tiers, where purely data-driven methods struggle.

### 5. Test-Time Adaptation Per Example is Highly Effective for Reasoning Tasks

The ARChitects' test-time LoRA finetuning (paper 13), enabled by the efficient LoRA formulation (paper 14), demonstrates that adapting the model to each specific test example at inference time yields substantial improvements (5-8% on ARC). This approach is particularly well-motivated for symbolic regression, where each equation to be derived has unique structural characteristics that a general-purpose model may not fully capture. PhysDiffuser's test-time LoRA adaptation trains rank-8 adapters for 32 steps on each new observation dataset, allowing the model to specialize its attention patterns and token predictions to the specific data distribution at hand. The low parameter count of LoRA adapters (approximately 500K) makes this feasible within CPU inference budgets.
