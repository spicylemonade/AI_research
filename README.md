# PhysMDT: Physics Masked Diffusion Transformer

A novel transformer-based architecture for autonomous discovery of physics equations from numerical observations, combining masked diffusion language models with physics-aware inductive biases.

## Key Results

| Tier | Description | Symbolic Accuracy | R² |
|------|-------------|------------------:|---:|
| 1 | Simple (F=ma, v=d/t) | 83.3% | 1.00 |
| 2 | Polynomial (KE=0.5mv²) | 43.3% | 0.84 |
| 3 | Rational (F=Gm1m2/r²) | 28.3% | 0.84 |
| 4 | Trig/Sqrt (pendulum) | 14.0% | 0.97 |
| 5 | Multi-step (Kepler) | 0.0% | 0.69 |
| **Overall** | | **40.0%** | **0.91** |

**Zero-shot discovery:** 1/11 held-out equations recovered (Lorentz force F=qvB, never seen in training)

## Architecture

```
Observations (x, y pairs) --> Set Transformer Encoder --> Observation Embeddings
                                                                  |
                                                          Cross-Attention
                                                                  |
Masked Expression Tokens --> Masked Diffusion Decoder --> Token Logits
         |                          |
    Tree-Positional          Dimensional Analysis
      Encoding                     Bias
```

**Novel components** (inspired by ARChitects ARC 2025 solution):
1. **Masked Diffusion Training**: Non-autoregressive, predicts all masked tokens simultaneously
2. **Recursive Soft-Masking Refinement**: 64-step iterative denoising with cosine unmasking
3. **Tree-Positional Encoding**: 2D learnable embeddings for expression tree structure
4. **Dimensional Analysis Bias**: Tracks mass/length/time consistency
5. **Test-Time Fine-Tuning**: LoRA adapters for per-problem adaptation

## Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Quick smoke test (~5 minutes)
./run_all.sh --quick

# Full pipeline (~1 hour on A100)
./run_all.sh
```

## Repository Structure

```
data/
  equations.py          # 61 physics equations (50 training + 11 held-out)
  tokenizer.py          # Prefix notation tokenizer (vocab=62)
  physics_generator.py  # Synthetic observation dataset
models/
  physmdt.py            # PhysMDT architecture (71.6M params)
  refinement.py         # Recursive soft-masking refinement
  ttft.py               # LoRA test-time fine-tuning
  ar_baseline.py        # Autoregressive baseline (33M params)
training/
  train_physmdt.py      # Curriculum training (3 phases)
  train_baseline.py     # AR baseline training
  evaluate_physmdt.py   # Full evaluation pipeline
  run_*.py              # Ablation, robustness, visualization scripts
evaluation/
  metrics.py            # Symbolic equivalence, R-squared, edit distance
paper/
  main.tex              # LaTeX paper draft
figures/                # Publication-quality figures (300 DPI)
results/                # JSON experiment results
sources.bib             # BibTeX references (16 entries)
run_all.sh              # Full pipeline script
requirements.txt        # Python dependencies
```

## Training

PhysMDT uses a 3-phase curriculum:
- **Phase 1**: Tier 1-2 equations (simple linear/polynomial)
- **Phase 2**: Tier 1-3 (adds rational expressions)
- **Phase 3**: Tier 1-4 (adds trig/sqrt compositions)

Masking ratio anneals from 90-100% to 30-100% over training.

```bash
# Full training
python training/train_physmdt.py \
    --n_samples_per_phase 50000 --batch_size 64 \
    --phase1_epochs 10 --phase2_epochs 10 --phase3_epochs 10

# Quick training
python training/train_physmdt.py --quick
```

**Hardware**: Single NVIDIA A100 (40GB), 0.62 hours, 3.17 GB peak memory

## Evaluation

```bash
# In-distribution + zero-shot discovery
python training/evaluate_physmdt.py --checkpoint checkpoints/physmdt_best.pt

# Ablation study
python training/run_ablation.py --checkpoint checkpoints/physmdt_best.pt

# Robustness evaluation
python training/run_robustness.py --checkpoint checkpoints/physmdt_best.pt
```

## Reproducibility

- **Random seed**: 42 (configurable via `--seed` or `SEED` env var)
- **All results** in `results/*.json` are from actual training and inference
- **No simulated results** - all numbers come from real model outputs
- **Model checkpoints** are excluded from git (regenerated by training)

## Citation

```bibtex
@article{physmdt2026,
  title={PhysMDT: Physics Equation Discovery via Masked Diffusion Transformers
         with Recursive Soft-Masking Refinement},
  year={2026},
  note={Single A100 GPU training. Code: this repository}
}
```

## License

Research use only.
