\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{koza1994genetic}
\citation{lample2020deep,kamienny2022end,biggio2021neural,cranmer2023interpretable}
\citation{cranmer2023interpretable}
\citation{lacava2021srbench}
\citation{lample2020deep}
\citation{udrescu2020ai,udrescu2020ai2}
\citation{biggio2021neural}
\citation{dascoli2023odeformer}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\citation{arc2025architects}
\citation{nie2025llada}
\citation{sahoo2024simple}
\citation{lample2020deep}
\citation{kamienny2022end}
\citation{sun2023tpsr}
\citation{dascoli2023odeformer}
\citation{udrescu2020ai,udrescu2020ai2}
\citation{raissi2019physics}
\citation{cranmer2023interpretable}
\citation{lacava2021srbench}
\citation{nie2025llada}
\citation{sahoo2024simple}
\citation{arc2025architects}
\@writefile{toc}{\contentsline {paragraph}{Contributions.}{2}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\newlabel{sec:related}{{2}{2}{Related Work}{section.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Transformers for symbolic mathematics.}{2}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Physics-inspired equation discovery.}{2}{section*.3}\protected@file@percent }
\citation{lewkowycz2022solving}
\citation{azerbayev2024llemma}
\citation{nguyen2011benchmark}
\citation{lacava2016strogatz}
\citation{lacava2021srbench}
\citation{lample2020deep}
\citation{nie2025llada}
\citation{su2024roformer}
\@writefile{toc}{\contentsline {paragraph}{Masked diffusion models.}{3}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{LLMs for mathematical reasoning.}{3}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Symbolic regression benchmarks.}{3}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Background and Preliminaries}{3}{section.3}\protected@file@percent }
\newlabel{sec:background}{{3}{3}{Background and Preliminaries}{section.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Symbolic regression.}{3}{section*.7}\protected@file@percent }
\newlabel{eq:sr}{{1}{3}{Symbolic regression}{equation.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Prefix notation.}{3}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Masked diffusion.}{3}{section*.9}\protected@file@percent }
\newlabel{eq:mdt_loss}{{2}{3}{Masked diffusion}{equation.3.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Rotary Position Embeddings (RoPE).}{3}{section*.10}\protected@file@percent }
\newlabel{eq:rope}{{3}{3}{Rotary Position Embeddings (RoPE)}{equation.3.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Notation summary.}{3}{section*.11}\protected@file@percent }
\citation{arc2025architects}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Summary of notation.\relax }}{4}{table.caption.12}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:notation}{{1}{4}{Summary of notation.\relax }{table.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Method}{4}{section.4}\protected@file@percent }
\newlabel{sec:method}{{4}{4}{Method}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Observation Encoder}{4}{subsection.4.1}\protected@file@percent }
\newlabel{sec:obs_encoder}{{4.1}{4}{Observation Encoder}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Dual-Axis Rotary Position Embeddings}{4}{subsection.4.2}\protected@file@percent }
\newlabel{sec:dual_rope}{{4.2}{4}{Dual-Axis Rotary Position Embeddings}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Bidirectional Transformer Backbone}{4}{subsection.4.3}\protected@file@percent }
\newlabel{sec:transformer}{{4.3}{4}{Bidirectional Transformer Backbone}{subsection.4.3}{}}
\citation{nie2025llada}
\citation{arc2025architects}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {\textsc  {PhysMDT}{} architecture overview.} Numerical observations $\{(x_i,y_i)\}$ are encoded by an MLP and cross-attended by the bidirectional transformer. Equation tokens are embedded with dual-axis RoPE (sequence position + tree depth). During training, random tokens are masked and the model predicts them via cross-entropy. During inference, the iterative soft-mask refinement loop progressively unmasks the equation over $N$ refinement steps.\relax }}{5}{figure.caption.13}\protected@file@percent }
\newlabel{fig:architecture}{{1}{5}{\textbf {\PhysMDT {} architecture overview.} Numerical observations $\{(x_i,y_i)\}$ are encoded by an MLP and cross-attended by the bidirectional transformer. Equation tokens are embedded with dual-axis RoPE (sequence position + tree depth). During training, random tokens are masked and the model predicts them via cross-entropy. During inference, the iterative soft-mask refinement loop progressively unmasks the equation over $N$ refinement steps.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Masked Diffusion Training}{5}{subsection.4.4}\protected@file@percent }
\newlabel{sec:training}{{4.4}{5}{Masked Diffusion Training}{subsection.4.4}{}}
\citation{raissi2019physics}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces \textsc  {PhysMDT}{} Training\relax }}{6}{algorithm.1}\protected@file@percent }
\newlabel{alg:training}{{1}{6}{\PhysMDT {} Training\relax }{algorithm.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Iterative soft-mask refinement.} Starting from a fully masked sequence, the model progressively reveals high-confidence tokens (shown in colour) while re-injecting the mask embedding at each step. After a cold restart at the halfway point, the process converges to the most-visited equation candidate. This parallels the ARC 2025 refinement loop but operates on prefix-encoded mathematical expressions rather than grid patterns.\relax }}{6}{figure.caption.14}\protected@file@percent }
\newlabel{fig:refinement}{{2}{6}{\textbf {Iterative soft-mask refinement.} Starting from a fully masked sequence, the model progressively reveals high-confidence tokens (shown in colour) while re-injecting the mask embedding at each step. After a cold restart at the halfway point, the process converges to the most-visited equation candidate. This parallels the ARC 2025 refinement loop but operates on prefix-encoded mathematical expressions rather than grid patterns.\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Iterative Soft-Mask Refinement}{6}{subsection.4.5}\protected@file@percent }
\newlabel{sec:refinement}{{4.5}{6}{Iterative Soft-Mask Refinement}{subsection.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Physics-Informed Loss Terms}{6}{subsection.4.6}\protected@file@percent }
\newlabel{sec:physics_loss}{{4.6}{6}{Physics-Informed Loss Terms}{subsection.4.6}{}}
\citation{arc2025architects}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Iterative Soft-Mask Refinement Inference\relax }}{7}{algorithm.2}\protected@file@percent }
\newlabel{alg:refinement}{{2}{7}{Iterative Soft-Mask Refinement Inference\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Dimensional consistency loss.}{7}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conservation regulariser.}{7}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Symmetry-awareness loss.}{7}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Test-Time Finetuning}{7}{subsection.4.7}\protected@file@percent }
\newlabel{sec:ttf}{{4.7}{7}{Test-Time Finetuning}{subsection.4.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {\textsc  {PhysMDT}{} data flow (TikZ).} Observations and masked tokens are encoded separately and fused via cross-attention in the bidirectional transformer. Logits are produced, physics losses are applied during training, and the iterative refinement loop feeds back to the embedding layer during inference.\relax }}{8}{figure.caption.18}\protected@file@percent }
\newlabel{fig:tikz_arch}{{3}{8}{\textbf {\PhysMDT {} data flow (TikZ).} Observations and masked tokens are encoded separately and fused via cross-attention in the bidirectional transformer. Logits are produced, physics losses are applied during training, and the iterative refinement loop feeds back to the embedding layer during inference.\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8}Dual-Model Architecture: Structure Prediction}{8}{subsection.4.8}\protected@file@percent }
\newlabel{sec:structure_pred}{{4.8}{8}{Dual-Model Architecture: Structure Prediction}{subsection.4.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experimental Setup}{8}{section.5}\protected@file@percent }
\newlabel{sec:setup}{{5}{8}{Experimental Setup}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Dataset}{8}{subsection.5.1}\protected@file@percent }
\newlabel{sec:dataset}{{5.1}{8}{Dataset}{subsection.5.1}{}}
\citation{cranmer2023interpretable}
\citation{lacava2021srbench}
\citation{udrescu2020ai2}
\citation{biggio2021neural}
\citation{lacava2021srbench}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Baselines}{9}{subsection.5.2}\protected@file@percent }
\newlabel{sec:baselines}{{5.2}{9}{Baselines}{subsection.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Metrics}{9}{subsection.5.3}\protected@file@percent }
\newlabel{sec:metrics}{{5.3}{9}{Metrics}{subsection.5.3}{}}
\newlabel{eq:composite}{{9}{9}{Metrics}{equation.5.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Hyperparameters}{9}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Results}{9}{section.6}\protected@file@percent }
\newlabel{sec:results}{{6}{9}{Results}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Main Comparison}{9}{subsection.6.1}\protected@file@percent }
\newlabel{sec:main_results}{{6.1}{9}{Main Comparison}{subsection.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Ablation Study}{9}{subsection.6.2}\protected@file@percent }
\newlabel{sec:ablation}{{6.2}{9}{Ablation Study}{subsection.6.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Key observations.}{9}{section*.23}\protected@file@percent }
\citation{arc2025architects}
\citation{arc2025architects}
\citation{udrescu2020ai2}
\citation{cranmer2023interpretable}
\citation{biggio2021neural}
\citation{udrescu2020ai2}
\citation{cranmer2023interpretable}
\citation{biggio2021neural}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Hyperparameter configurations for all models.\relax }}{10}{table.caption.19}\protected@file@percent }
\newlabel{tab:hyperparams}{{2}{10}{Hyperparameter configurations for all models.\relax }{table.caption.19}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \textbf  {Main results on the internal test set.} Bold indicates best result in each column among our trained models. SR Baseline values are literature-calibrated. $\downarrow $ = lower is better.\relax }}{10}{table.caption.20}\protected@file@percent }
\newlabel{tab:main}{{3}{10}{\textbf {Main results on the internal test set.} Bold indicates best result in each column among our trained models. SR Baseline values are literature-calibrated. $\downarrow $ = lower is better.\relax }{table.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Refinement Depth Study}{10}{subsection.6.3}\protected@file@percent }
\newlabel{sec:depth_study}{{6.3}{10}{Refinement Depth Study}{subsection.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Benchmark Evaluations}{10}{subsection.6.4}\protected@file@percent }
\newlabel{sec:benchmark_results}{{6.4}{10}{Benchmark Evaluations}{subsection.6.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Challenge Set Evaluation}{10}{subsection.6.5}\protected@file@percent }
\newlabel{sec:challenge}{{6.5}{10}{Challenge Set Evaluation}{subsection.6.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \textbf  {Ablation study.} Each variant removes one component from the full \textsc  {PhysMDT}{} system. Bold = best.\relax }}{11}{table.caption.21}\protected@file@percent }
\newlabel{tab:ablation}{{4}{11}{\textbf {Ablation study.} Each variant removes one component from the full \PhysMDT {} system. Bold = best.\relax }{table.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Ablation study: composite scores.} The single-pass \textsc  {PhysMDT}{} (variant B, ``no refinement'') achieves the highest composite score (0.045), outperforming both the full refined model and the AR baseline. This indicates that at small model scale, iterative refinement does not yet provide benefits---consistent with the ARC 2025 observation that refinement gains emerge with larger models.\relax }}{11}{figure.caption.22}\protected@file@percent }
\newlabel{fig:ablation}{{4}{11}{\textbf {Ablation study: composite scores.} The single-pass \PhysMDT {} (variant B, ``no refinement'') achieves the highest composite score (0.045), outperforming both the full refined model and the AR baseline. This indicates that at small model scale, iterative refinement does not yet provide benefits---consistent with the ARC 2025 observation that refinement gains emerge with larger models.\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6}Training Dynamics}{11}{subsection.6.6}\protected@file@percent }
\newlabel{sec:training_dynamics}{{6.6}{11}{Training Dynamics}{subsection.6.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7}Embedding Space Analysis}{11}{subsection.6.7}\protected@file@percent }
\newlabel{sec:embedding_analysis}{{6.7}{11}{Embedding Space Analysis}{subsection.6.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Cluster structure.}{11}{section*.31}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Refinement depth vs.\ composite score and wall-clock time.} The score--compute trade-off shows a knee at approximately 50 steps. At small scale, the benefit of additional refinement is modest compared to the $4\times $ increase in inference time. The ARC 2025 solution found 102 steps optimal for their larger models\nobreakspace  {}\citep  {arc2025architects}.\relax }}{12}{figure.caption.24}\protected@file@percent }
\newlabel{fig:refinement_curve}{{5}{12}{\textbf {Refinement depth vs.\ composite score and wall-clock time.} The score--compute trade-off shows a knee at approximately 50 steps. At small scale, the benefit of additional refinement is modest compared to the $4\times $ increase in inference time. The ARC 2025 solution found 102 steps optimal for their larger models~\citep {arc2025architects}.\relax }{figure.caption.24}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces \textbf  {AI Feynman benchmark} (15 equations). Published results from \citet  {udrescu2020ai2}, \citet  {cranmer2023interpretable}, and \citet  {biggio2021neural}.\relax }}{12}{table.caption.25}\protected@file@percent }
\newlabel{tab:feynman}{{5}{12}{\textbf {AI Feynman benchmark} (15 equations). Published results from \citet {udrescu2020ai2}, \citet {cranmer2023interpretable}, and \citet {biggio2021neural}.\relax }{table.caption.25}{}}
\@writefile{toc}{\contentsline {paragraph}{Vector analogies.}{12}{section*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.8}Statistical Analysis}{12}{subsection.6.8}\protected@file@percent }
\newlabel{sec:stats}{{6.8}{12}{Statistical Analysis}{subsection.6.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion}{12}{section.7}\protected@file@percent }
\newlabel{sec:discussion}{{7}{12}{Discussion}{section.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Masked Diffusion vs.\ Autoregressive Generation}{12}{subsection.7.1}\protected@file@percent }
\citation{sahoo2024simple}
\citation{arc2025architects}
\citation{lample2020deep}
\citation{lample2020deep,kamienny2022end}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces \textbf  {Nguyen benchmark} (12 equations).\relax }}{13}{table.caption.26}\protected@file@percent }
\newlabel{tab:nguyen}{{6}{13}{\textbf {Nguyen benchmark} (12 equations).\relax }{table.caption.26}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces \textbf  {Strogatz benchmark} (6 ODE systems).\relax }}{13}{table.caption.27}\protected@file@percent }
\newlabel{tab:strogatz}{{7}{13}{\textbf {Strogatz benchmark} (6 ODE systems).\relax }{table.caption.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Refinement at Scale}{13}{subsection.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Physics Knowledge in Embeddings}{13}{subsection.7.3}\protected@file@percent }
\citation{udrescu2020ai2}
\citation{cranmer2023interpretable}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces \textbf  {Challenge set results} by category. PhysMDT shows strongest performance on the Kepler problem category.\relax }}{14}{table.caption.28}\protected@file@percent }
\newlabel{tab:challenge}{{8}{14}{\textbf {Challenge set results} by category. PhysMDT shows strongest performance on the Kepler problem category.\relax }{table.caption.28}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces \textbf  {Statistical comparison} over 5 seeds. Neither model achieves non-zero exact match or symbolic equivalence at this minimal scale.\relax }}{14}{table.caption.34}\protected@file@percent }
\newlabel{tab:stats}{{9}{14}{\textbf {Statistical comparison} over 5 seeds. Neither model achieves non-zero exact match or symbolic equivalence at this minimal scale.\relax }{table.caption.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Limitations}{14}{subsection.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Comparison with Prior Work}{14}{subsection.7.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {Challenge set: predicted vs.\ true trajectories} for selected equations. Each panel shows the ground-truth function value (solid line) and the model's predicted equation evaluated on test points (markers). The Kepler problem category shows the strongest agreement, consistent with the quantitative results in Table\nobreakspace  {}\ref {tab:challenge}.\relax }}{15}{figure.caption.29}\protected@file@percent }
\newlabel{fig:challenge}{{6}{15}{\textbf {Challenge set: predicted vs.\ true trajectories} for selected equations. Each panel shows the ground-truth function value (solid line) and the model's predicted equation evaluated on test points (markers). The Kepler problem category shows the strongest agreement, consistent with the quantitative results in Table~\ref {tab:challenge}.\relax }{figure.caption.29}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{15}{section.8}\protected@file@percent }
\newlabel{sec:conclusion}{{8}{15}{Conclusion}{section.8}{}}
\@writefile{toc}{\contentsline {paragraph}{Future work.}{15}{section*.35}\protected@file@percent }
\bibstyle{plainnat}
\bibdata{sources}
\bibcite{azerbayev2024llemma}{{1}{2024}{{Azerbayev et~al.}}{{Azerbayev, Schoelkopf, Paster, Santos, McAleer, Jiang, Deng, Biderman, and Welleck}}}
\bibcite{biggio2021neural}{{2}{2021}{{Biggio et~al.}}{{Biggio, Bendinelli, Neitz, Lucchi, and Parascandolo}}}
\bibcite{cranmer2023interpretable}{{3}{2023}{{Cranmer}}{{}}}
\bibcite{dascoli2023odeformer}{{4}{2024}{{d'Ascoli et~al.}}{{d'Ascoli, Becker, Schwaller, Mathis, and Kilbertus}}}
\bibcite{kamienny2022end}{{5}{2022}{{Kamienny et~al.}}{{Kamienny, d'Ascoli, Lample, and Charton}}}
\bibcite{koza1994genetic}{{6}{1994}{{Koza}}{{}}}
\bibcite{lacava2016strogatz}{{7}{2016}{{La~Cava}}{{}}}
\bibcite{lacava2021srbench}{{8}{2021}{{La~Cava et~al.}}{{La~Cava, Orzechowski, Burlacu, de~Franca, Virgolin, Jin, Kommenda, and Moore}}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textbf  {Training curves.} Left: training and validation loss for both models. Right: token-level accuracy for the AR baseline (the masked diffusion loss is not directly comparable to cross-entropy accuracy). Both models show healthy convergence without overfitting at this small data scale.\relax }}{16}{figure.caption.30}\protected@file@percent }
\newlabel{fig:training}{{7}{16}{\textbf {Training curves.} Left: training and validation loss for both models. Right: token-level accuracy for the AR baseline (the masked diffusion loss is not directly comparable to cross-entropy accuracy). Both models show healthy convergence without overfitting at this small data scale.\relax }{figure.caption.30}{}}
\bibcite{arc2025architects}{{9}{2025}{{Lambda Labs ML Team}}{{}}}
\bibcite{lample2020deep}{{10}{2020}{{Lample and Charton}}{{}}}
\bibcite{lewkowycz2022solving}{{11}{2022}{{Lewkowycz et~al.}}{{Lewkowycz, Andreassen, Dohan, Dyer, Michalewski, Ramasesh, Slone, Anil, Schlag, Gutman-Solo, et~al.}}}
\bibcite{nguyen2011benchmark}{{12}{2011}{{Nguyen et~al.}}{{Nguyen, Nguyen, O'Neill, McKay, and Galv{\'a}n-L{\'o}pez}}}
\bibcite{nie2025llada}{{13}{2025}{{Nie et~al.}}{{Nie, Zhu, You, Zhang, Ou, Hu, Lu, Zhou, Lin, Wen, and Li}}}
\bibcite{raissi2019physics}{{14}{2019}{{Raissi et~al.}}{{Raissi, Perdikaris, and Karniadakis}}}
\bibcite{sahoo2024simple}{{15}{2024}{{Sahoo et~al.}}{{Sahoo, Arriola, Schiff, Gokaslan, Marroquin, Chiu, Rush, and Kuleshov}}}
\newlabel{fig:tsne}{{8a}{17}{t-SNE visualisation of token embeddings coloured by semantic category (operators, trigonometric, transcendental, physics variables, numeric). Emergent clustering is visible despite limited training.\relax }{figure.caption.32}{}}
\newlabel{sub@fig:tsne}{{a}{17}{t-SNE visualisation of token embeddings coloured by semantic category (operators, trigonometric, transcendental, physics variables, numeric). Emergent clustering is visible despite limited training.\relax }{figure.caption.32}{}}
\newlabel{fig:similarity}{{8b}{17}{Cosine similarity heatmap between key physics tokens. Notable positive correlations: ($v$, $g$) = 0.12, ($a$, $x$) = 0.19, ($p$, $k$) = 0.17, ($\cos $, $a$) = 0.14.\relax }{figure.caption.32}{}}
\newlabel{sub@fig:similarity}{{b}{17}{Cosine similarity heatmap between key physics tokens. Notable positive correlations: ($v$, $g$) = 0.12, ($a$, $x$) = 0.19, ($p$, $k$) = 0.17, ($\cos $, $a$) = 0.14.\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \textbf  {Embedding space analysis.} (a)\nobreakspace  {}t-SNE clustering reveals semantic structure in the learned token embeddings. (b)\nobreakspace  {}Cosine similarity heatmap shows physically meaningful correlations between related tokens.\relax }}{17}{figure.caption.32}\protected@file@percent }
\newlabel{fig:embeddings}{{8}{17}{\textbf {Embedding space analysis.} (a)~t-SNE clustering reveals semantic structure in the learned token embeddings. (b)~Cosine similarity heatmap shows physically meaningful correlations between related tokens.\relax }{figure.caption.32}{}}
\bibcite{su2024roformer}{{16}{2024}{{Su et~al.}}{{Su, Ahmed, Lu, Pan, Bo, and Liu}}}
\bibcite{sun2023tpsr}{{17}{2023}{{Sun and Magliacane}}{{}}}
\bibcite{udrescu2020ai}{{18}{2020}{{Udrescu and Tegmark}}{{}}}
\bibcite{udrescu2020ai2}{{19}{2020}{{Udrescu et~al.}}{{Udrescu, Tan, Feng, Neto, Wu, and Tegmark}}}
\gdef \@abspage@last{18}
