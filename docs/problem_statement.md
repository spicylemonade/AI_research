# Problem Statement: Deriving Newtonian Physics via Masked Diffusion Transformers

## 1. Formal Problem Definition

### Symbolic Regression as Masked Sequence Prediction

Given a dataset D = {(x_i, y_i)}_{i=1}^{N} where x_i ∈ ℝ^d are input variable vectors and y_i ∈ ℝ are output values generated by an unknown function f* such that y_i = f*(x_i) + ε_i (with ε_i ~ N(0, σ²)), the goal of symbolic regression is to recover a symbolic expression f̂ that is functionally equivalent to f*.

We formulate this as a **sequence-to-sequence masked prediction** task:

1. **Input encoding**: The dataset D is encoded into a fixed-size representation z ∈ ℝ^{d_model} via a set encoder: z = Enc({(x_i, y_i)}_{i=1}^{N}).

2. **Target representation**: The symbolic expression f* is tokenized into a sequence of tokens in reverse Polish notation (RPN): s = (s_1, s_2, ..., s_L) where s_j ∈ V (vocabulary of operators, variables, and constants).

3. **Masked diffusion objective**: During training, tokens are randomly masked with probability p ~ Uniform(0, 1): s̃_j = [MASK] with probability p, else s̃_j = s_j. The model learns p(s_j | s̃, z) — the conditional distribution of each masked token given the partially masked sequence and the data encoding.

4. **Loss function**: Cross-entropy on masked positions only:
   L = -Σ_{j ∈ M} log p_θ(s_j | s̃, z)
   where M = {j : s̃_j = [MASK]} is the set of masked positions.

5. **Inference via soft-masking recursion**: Starting from a fully masked sequence, iteratively refine predictions through N refinement steps without hard discretization between steps, producing the final equation prediction.

## 2. Target Newtonian Equations

We enumerate 18 target equations spanning four domains of classical mechanics, annotated with complexity (number of variables, operators, and nesting depth):

### Mechanics (5 equations)

| # | Name | Equation | Variables | Operators | Depth |
|---|------|----------|-----------|-----------|-------|
| 1 | Newton's second law | F = m·a | 2 | 1 | 1 |
| 2 | Kinetic energy | E_k = ½·m·v² | 2 | 3 | 2 |
| 3 | Projectile range | R = (v₀²·sin(2θ))/g | 3 | 5 | 3 |
| 4 | Work-energy theorem | W = F·d·cos(θ) | 3 | 3 | 2 |
| 5 | Centripetal acceleration | a_c = v²/r | 2 | 2 | 2 |

### Gravitation (4 equations)

| # | Name | Equation | Variables | Operators | Depth |
|---|------|----------|-----------|-----------|-------|
| 6 | Gravitational force | F = G·m₁·m₂/r² | 4 | 4 | 2 |
| 7 | Gravitational PE | U = -G·m₁·m₂/r | 4 | 4 | 2 |
| 8 | Orbital velocity | v = sqrt(G·M/r) | 3 | 3 | 3 |
| 9 | Kepler's third law | T² = (4π²/GM)·a³ | 3 | 5 | 3 |

### Oscillations (5 equations)

| # | Name | Equation | Variables | Operators | Depth |
|---|------|----------|-----------|-----------|-------|
| 10 | Simple harmonic oscillator | x(t) = A·cos(ω·t + φ) | 4 | 4 | 3 |
| 11 | Damped oscillator | x(t) = A·exp(-γ·t)·cos(ω_d·t) | 4 | 6 | 4 |
| 12 | Driven oscillator amplitude | A = F₀/sqrt((ω₀²-ω²)² + (2γω)²) | 4 | 10 | 5 |
| 13 | Pendulum period | T = 2π·sqrt(L/g) | 2 | 4 | 3 |
| 14 | Spring PE | U = ½·k·x² | 2 | 3 | 2 |

### Conservation Laws & Advanced (4 equations)

| # | Name | Equation | Variables | Operators | Depth |
|---|------|----------|-----------|-----------|-------|
| 15 | Angular momentum | L = I·ω = m·r²·ω | 3 | 3 | 2 |
| 16 | Moment of inertia (rod) | I = (1/12)·m·L² | 2 | 3 | 2 |
| 17 | Coupled oscillation (normal mode) | ω± = sqrt((k₁+k₂)/m ± k₂/m) | 3 | 6 | 4 |
| 18 | Euler-Lagrange (free particle KE) | T = ½·m·(ẋ² + ẏ²) | 3 | 5 | 3 |

**Complexity summary**: Variable count ranges from 2 to 4; operator count from 1 to 10; nesting depth from 1 to 5. Equations include nested trigonometric functions (sin, cos), exponentials, square roots, and rational expressions.

## 3. Research Hypotheses

### H1: Soft-Masking Recursion Superiority
**Statement**: Soft-masking recursion with ≥50 refinement steps improves symbolic solution rate by ≥10 percentage points over single-pass autoregressive decoding on the SRSD-Feynman benchmark.

**Rationale**: The ARChitects [architects2025arc] demonstrated that soft-masking recursion is the single most impactful innovation in their ARC solution. Autoregressive models like SymbolicGPT [valipour2021symbolicgpt] commit to each token sequentially with no ability to revise. Masked diffusion with iterative refinement allows the model to globally reconsider all positions simultaneously, correcting early mistakes. This is especially valuable for mathematical expressions where a single wrong operator can invalidate the entire equation.

**Falsification**: Run PhysMDT with and without soft-masking recursion (using greedy single-pass decoding as control) on SRSD-Feynman and compare solution rates with paired statistical test (p < 0.05).

### H2: Tree-Aware Positional Encoding Captures Mathematical Structure
**Statement**: Tree-aware 2D positional encoding (inspired by Golden Gate RoPE [architects2025arc, su2021rope]) improves solution rate by ≥5 percentage points compared to standard 1D sinusoidal positional encoding, with the largest gains on equations with nesting depth ≥ 3.

**Rationale**: Mathematical expressions have inherent tree structure (operator precedence, nesting). Standard 1D position encoding treats the token sequence as flat, losing structural information. Golden Gate RoPE demonstrated in ARC tasks that encoding multi-directional positional information improves pattern recognition. Adapting this to expression trees should provide the transformer with direct awareness of mathematical structure, particularly beneficial for deeply nested equations.

**Falsification**: Ablation experiment comparing PhysMDT with tree-aware PE vs. standard sinusoidal PE on SRSD-Feynman, stratified by equation complexity. Measure solution rate difference on high-depth (≥3) vs. low-depth (<3) equations.

### H3: Test-Time Finetuning Enables Per-Equation Specialization
**Statement**: Per-equation test-time finetuning with LoRA (128 steps, rank 32) improves R² by ≥0.05 on average across SRSD-Feynman test equations compared to inference without TTF.

**Rationale**: Each physics equation has unique numerical patterns in its data. TTF allows the model to adapt its internal representations to the specific data distribution at inference time, analogous to how the ARChitects' per-task TTF [architects2025arc] dramatically improved ARC performance. This is especially important for equations with unusual constants or non-standard variable ranges.

**Falsification**: Compare PhysMDT evaluation with and without TTF on the same test set. Report per-equation R² improvement and compute significance via paired t-test.

### H4: Masked Diffusion Outperforms Autoregressive for Symbolic Regression
**Statement**: PhysMDT (masked diffusion + soft-masking + tree PE + TTF) achieves a higher overall solution rate than all published transformer-based autoregressive methods (SymbolicGPT, E2E-Transformer, TPSR) on SRSD-Feynman, establishing a new SOTA among neural symbolic regression methods.

**Rationale**: Current transformer-based SR methods [valipour2021symbolicgpt, kamienny2022e2e, shojaee2023tpsr] all use autoregressive decoding, which suffers from: (1) left-to-right generation bias, (2) no ability to revise earlier tokens, (3) exposure bias from teacher forcing. Masked diffusion addresses all three limitations while the soft-masking recursion provides iterative refinement capability not available to AR models. Combined with tree-aware PE and TTF, this should yield SOTA neural SR performance.

**Falsification**: Compare against published numbers from at least 4 baselines on the same benchmark. Our method must exceed the best published transformer-based result (TPSR ~58% overall on FSReD) with statistical significance.

## 4. Novel Contributions

### Gap 1: No masked diffusion model has been applied to symbolic regression
All prior transformer-based SR methods [valipour2021symbolicgpt, kamienny2022e2e, biggio2021nesymres, shojaee2023tpsr, dascoli2024odeformer] use autoregressive decoding. We are the first to adapt masked diffusion (pioneered by LLaDA [nie2025llada] and MDLM [sahoo2024mdlm]) to the symbolic regression domain.

### Gap 2: Soft-masking recursion has not been explored for mathematical expression generation
The soft-masking recursion mechanism [architects2025arc] was developed for 2D grid puzzles (ARC). We adapt it to 1D symbolic token sequences, demonstrating that the iterative refinement paradigm transfers to equation discovery where the model can progressively correct mathematical structure.

### Gap 3: No positional encoding captures expression tree structure for transformers
Existing SR transformers use standard 1D positional encodings (sinusoidal or learned). We introduce tree-aware 2D positional encoding that adapts Golden Gate RoPE [su2021rope, architects2025arc] from 2D grid structure to expression tree structure, providing the first position encoding that directly encodes mathematical hierarchy.

### Gap 4: Test-time finetuning has not been combined with neural symbolic regression
TTF with LoRA [hu2022lora] has shown dramatic improvements in ARC [architects2025arc] and other domains, but no SR method employs per-equation adaptation at inference time. We demonstrate that TTF enables equation-specific specialization that significantly boosts performance, especially on harder equations with unusual numerical patterns.

### Gap 5: No unified framework combining all four innovations
Each innovation (masked diffusion, soft-masking recursion, tree-aware PE, TTF) individually addresses a distinct limitation of current SR methods. PhysMDT is the first system to combine all four in a unified framework, with each component shown to contribute independently through ablation studies.
