% Bibliography for PhysDiffuse research project
% Maintained throughout the project â€” every web source, paper, or code reference must be recorded here

@article{udrescu2020aifeynman,
  author    = {Silviu-Marian Udrescu and Max Tegmark},
  title     = {{AI Feynman}: A Physics-Inspired Method for Symbolic Regression},
  journal   = {Science Advances},
  volume    = {6},
  number    = {16},
  pages     = {eaay2631},
  year      = {2020},
  doi       = {10.1126/sciadv.aay2631},
}

@article{udrescu2020aifeynman2,
  author    = {Silviu-Marian Udrescu and Andrew Tan and Jiahai Feng and Orisvaldo Neto and Tailin Wu and Max Tegmark},
  title     = {{AI Feynman 2.0}: Pareto-Optimal Symbolic Regression Exploiting Graph Modularity},
  journal   = {Advances in Neural Information Processing Systems},
  volume    = {33},
  pages     = {4860--4871},
  year      = {2020},
}

@inproceedings{kamienny2022e2esr,
  author    = {Pierre-Alexandre Kamienny and St\'{e}phane d'Ascoli and Guillaume Lample and Fran\c{c}ois Charton},
  title     = {End-to-End Symbolic Regression with Transformers},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {35},
  year      = {2022},
}

@inproceedings{biggio2021nesymres,
  author    = {Luca Biggio and Tommaso Bendinelli and Alexander Neitz and Aurelien Lucchi and Giambattista Parascandolo},
  title     = {Neural Symbolic Regression that Scales},
  booktitle = {International Conference on Machine Learning (ICML)},
  pages     = {936--945},
  year      = {2021},
  publisher = {PMLR},
}

@article{valipour2021symbolicgpt,
  author    = {Mojtaba Valipour and Bowen You and Maysum Panju and Ali Ghodsi},
  title     = {{SymbolicGPT}: A Generative Transformer Model for Symbolic Regression},
  journal   = {arXiv preprint arXiv:2106.14131},
  year      = {2021},
}

@inproceedings{shojaee2023tpsr,
  author    = {Parshin Shojaee and Kazem Meidani and Amir Barati Farimani and Chandan Reddy},
  title     = {{TPSR}: Transformer-based Planning for Symbolic Regression},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {36},
  pages     = {45907--45919},
  year      = {2023},
}

@article{meidani2023snip,
  author    = {Kazem Meidani and Parshin Shojaee and Chandan K. Reddy and Amir Barati Farimani},
  title     = {{SNIP}: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training},
  journal   = {arXiv preprint arXiv:2310.02227},
  year      = {2023},
}

@inproceedings{mdlformer2025,
  author    = {Zhengtao Li and others},
  title     = {Symbolic Regression via {MDLformer}-Guided Search: From Minimizing Prediction Error to Minimizing Description Length},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2025},
}

@article{nie2025llada,
  author    = {Shen Nie and Fengqi Zhu and Zebin You and Xiaolu Zhang and Jingyang Ou and Jun Hu and Jun Zhou and Yankai Lin and Ji-Rong Wen and Chongxuan Li},
  title     = {{LLaDA}: Large Language Diffusion Models},
  journal   = {arXiv preprint arXiv:2502.09992},
  year      = {2025},
}

@inproceedings{sahoo2024mdlm,
  author    = {Subham Sekhar Sahoo and Marianne Arriola and Aaron Gokaslan and Edgar Mariano Marroquin and Alexander M. Rush and Yair Schiff and Justin T. Chiu and Volodymyr Kuleshov},
  title     = {Simple and Effective Masked Diffusion Language Models},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2024},
}

@article{ddsr2025,
  author    = {Anonymous},
  title     = {Diffusion-Based Symbolic Regression},
  journal   = {arXiv preprint arXiv:2505.24776},
  year      = {2025},
}

@article{lomdm2025,
  author    = {Anonymous},
  title     = {Unifying Masked Diffusion Models with Various Generation Orders and Beyond},
  journal   = {arXiv preprint arXiv:2602.02112},
  year      = {2025},
}

@article{ying2025phye2e,
  author    = {Jie Ying and others},
  title     = {A Neural Symbolic Model for Space Physics},
  journal   = {Nature Machine Intelligence},
  year      = {2025},
  doi       = {10.1038/s42256-025-01126-3},
}

@article{akyurek2024ttt,
  author    = {Ekin Aky\"{u}rek and others},
  title     = {The Surprising Effectiveness of Test-Time Training for Few-Shot Learning},
  journal   = {arXiv preprint arXiv:2411.07279},
  year      = {2024},
}

@article{cranmer2023pysr,
  author    = {Miles Cranmer},
  title     = {Interpretable Machine Learning for Science with {PySR} and {SymbolicRegression.jl}},
  journal   = {arXiv preprint arXiv:2305.01582},
  year      = {2023},
}

@article{matsubara2022srsd,
  author    = {Yoshitomo Matsubara and Naoya Chiba and Ryo Igarashi and Yoshitaka Ushiku},
  title     = {Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery},
  journal   = {Journal of Data-centric Machine Learning Research (DMLR)},
  year      = {2022},
}

@misc{arc2025architects,
  author    = {Lambda Labs and The ARChitects Team},
  title     = {{ARC2025} Solution by the {ARChitects}: Recursive Soft-Masking with {LLaDA}},
  howpublished = {\url{https://lambdalabsml.github.io/ARC2025_Solution_by_the_ARChitects/}},
  year      = {2025},
}

@inproceedings{holt2023dgsr,
  author    = {Samuel Holt and Zhaozhi Qian and Mihaela van der Schaar},
  title     = {Deep Generative Symbolic Regression},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2023},
}

@article{vaswani2017attention,
  author    = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and {\L}ukasz Kaiser and Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {Advances in Neural Information Processing Systems},
  volume    = {30},
  year      = {2017},
}

@inproceedings{lee2019settransformer,
  author    = {Juho Lee and Yoonho Lee and Jungtaek Kim and Adam Kosiorek and Seungjin Choi and Yee Whye Teh},
  title     = {Set Transformer: A Framework for Attention-based Permutation-Invariant Input},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2019},
}

@article{hu2022lora,
  author    = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
  title     = {{LoRA}: Low-Rank Adaptation of Large Language Models},
  journal   = {International Conference on Learning Representations (ICLR)},
  year      = {2022},
}
