% Bibliography for research project
% PhysMDT: Physics Masked Diffusion Transformer

@article{valipour2021symbolicgpt,
  title={SymbolicGPT: A Generative Transformer Model for Symbolic Regression},
  author={Valipour, Mojtaba and You, Bowen and Panju, Maysum and Ghodsi, Ali},
  journal={arXiv preprint arXiv:2106.14131},
  year={2021},
  url={https://arxiv.org/abs/2106.14131}
}

@article{udrescu2020aifeynman,
  title={AI Feynman: A Physics-Inspired Method for Symbolic Regression},
  author={Udrescu, Silviu-Marian and Tegmark, Max},
  journal={Science Advances},
  volume={6},
  number={16},
  pages={eaay2631},
  year={2020},
  publisher={AAAS},
  url={https://arxiv.org/abs/1905.11481}
}

@inproceedings{udrescu2020aifeynman2,
  title={AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity},
  author={Udrescu, Silviu-Marian and Tan, Andrew and Feng, Jiahai and Neto, Orisvaldo and Wu, Tailin and Tegmark, Max},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={33},
  year={2020},
  url={https://arxiv.org/abs/2006.10782}
}

@article{ying2025phye2e,
  title={A Neural Symbolic Model for Space Physics},
  author={Ying, Jie and others},
  journal={Nature Machine Intelligence},
  year={2025},
  url={https://arxiv.org/abs/2503.07994}
}

@inproceedings{dascoli2024odeformer,
  title={ODEFormer: Symbolic Regression of Dynamical Systems with Transformers},
  author={d'Ascoli, St{\'e}phane and Becker, S{\"o}ren and Mathis, Alexander and Schwaller, Philippe and Kilbertus, Niki},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024},
  url={https://arxiv.org/abs/2310.05573}
}

@inproceedings{shojaee2023tpsr,
  title={Transformer-based Planning for Symbolic Regression},
  author={Shojaee, Parshin and Meidani, Kazem and Farimani, Amir Barati and Reddy, Chandan},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={36},
  year={2023},
  url={https://arxiv.org/abs/2303.06833}
}

@article{nie2025llada,
  title={Large Language Diffusion Models},
  author={Nie, Shen and Zhu, Fengqi and others},
  journal={arXiv preprint arXiv:2502.09992},
  year={2025},
  url={https://arxiv.org/abs/2502.09992}
}

@misc{architects2025arc,
  title={The ARChitects -- ARC Prize 2025 Solution Technical Report},
  author={{The ARChitects Team, Lambda Labs}},
  year={2025},
  url={https://lambdalabsml.github.io/ARC2025_Solution_by_the_ARChitects/}
}

@inproceedings{kamienny2022e2e,
  title={End-to-end Symbolic Regression with Transformers},
  author={Kamienny, Pierre-Alexandre and d'Ascoli, St{\'e}phane and Lample, Guillaume and Charton, Fran{\c{c}}ois},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022},
  url={https://arxiv.org/abs/2204.10532}
}

@inproceedings{biggio2021nesymres,
  title={Neural Symbolic Regression that Scales},
  author={Biggio, Luca and Bendinelli, Tommaso and Neitz, Alexander and Lucchi, Aurelien and Parascandolo, Giambattista},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2021},
  url={https://arxiv.org/abs/2106.06427}
}

@article{tian2024symq,
  title={Interactive Symbolic Regression through Offline Reinforcement Learning: A Co-Design Framework},
  author={Tian, Yuan and Zhou, Wenqi and Dong, Hao and Kammer, David S. and Fink, Olga},
  journal={Nature Communications},
  year={2025},
  url={https://arxiv.org/abs/2402.05306}
}

@inproceedings{shojaee2025llmsr,
  title={LLM-SR: Scientific Equation Discovery via Programming with Large Language Models},
  author={Shojaee, Parshin and Meidani, Kazem and Gupta, Shashank and Farimani, Amir Barati and Reddy, Chandan K.},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2025},
  url={https://arxiv.org/abs/2404.18400}
}

@inproceedings{sahoo2024mdlm,
  title={Simple and Effective Masked Diffusion Language Models},
  author={Sahoo, Subham Sekhar and Arriola, Marianne and Gokaslan, Aaron and Marroquin, Edgar Mariano and Rush, Alexander M. and Schiff, Yair and Chiu, Justin T. and Kuleshov, Volodymyr},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2024},
  url={https://arxiv.org/abs/2406.07524}
}

@inproceedings{lacava2021srbench,
  title={Contemporary Symbolic Regression Methods and their Relative Performance},
  author={La Cava, William and Orzechowski, Patryk and Burlacu, Bogdan and de Fran{\c{c}}a, Fabr{\'\i}cio Olivetti and Virgolin, Marco and Jin, Ying and Kommenda, Michael and Moore, Jason H.},
  booktitle={NeurIPS Track on Datasets and Benchmarks},
  year={2021},
  url={https://arxiv.org/abs/2107.14351}
}

@article{matsubara2023srsd,
  title={Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery},
  author={Matsubara, Yoshitomo and Chiba, Naoya and Igarashi, Ryo and Ushiku, Yoshitaka},
  journal={Journal of Data-centric Machine Learning Research (DMLR)},
  year={2023},
  url={https://arxiv.org/abs/2206.10540}
}

@article{zheng2023mdtv2,
  title={MDTv2: Masked Diffusion Transformer is a Strong Image Synthesizer},
  author={Zheng, Shanghua and others},
  journal={arXiv preprint arXiv:2303.14389},
  year={2023},
  url={https://arxiv.org/abs/2303.14389}
}

@article{zheng2024maskdit,
  title={Fast Training of Diffusion Models with Masked Transformers},
  author={Zheng, Hongkai and others},
  journal={arXiv preprint arXiv:2306.09305},
  year={2024},
  url={https://arxiv.org/abs/2306.09305}
}

@article{su2021rope,
  title={RoFormer: Enhanced Transformer with Rotary Position Embedding},
  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
  journal={arXiv preprint arXiv:2104.09864},
  year={2021},
  url={https://arxiv.org/abs/2104.09864}
}

@inproceedings{hu2022lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2022},
  url={https://arxiv.org/abs/2106.09685}
}

@article{llmsrbench2025,
  title={LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models},
  author={Shojaee, Parshin and others},
  journal={arXiv preprint arXiv:2504.10415},
  year={2025},
  url={https://arxiv.org/abs/2504.10415}
}
